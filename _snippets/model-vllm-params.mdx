| 参数                | 类型                          | 默认值                      | 描述                                                                                   |
| ------------------- | ----------------------------- | ---------------------------- | --------------------------------------------------------------------------------------------- |
| `id`                | `str`                         | **必填**                 | 要使用的模型的 ID (例如 `"Qwen/Qwen2.5-7B-Instruct"`)。 |
| `name`              | `str`                         | `"vLLM"`                     | 此模型实例的名称。                                                                   |
| `provider`          | `str`                         | `"vLLM"`                     | 提供商名称。                                                                                |
| `api_key`           | `Optional[str]`               | `"EMPTY"`                    | API 密钥 (为符合 OpenAI 的兼容性而发送；通常不需要)。                              |
| `base_url`          | `str`                         | `"http://localhost:8000/v1/"` | vLLM 服务器的 URL (兼容 OpenAI 的端点)。                                          |
| `max_tokens`        | `Optional[int]`               | `None`                       | 要生成的最大 token 数。                                                                    |
| `temperature`       | `float`                       | `0.7`                        | 采样温度。                                                                          |
| `top_p`             | `float`                       | `0.8`                        | 核心采样概率。                                                                    |
| `top_k`             | `Optional[int]`               | `None`                       | 将采样限制为前 K 个 token。                                                         |
| `frequency_penalty` | `Optional[float]`             | `None`                       | 根据 token 在文本中出现的频率对新 token 进行惩罚。                           |
| `presence_penalty`  | `float`                       | `1.5`                        | 重复惩罚。                                                                            |
| `stop`              | `Optional[Union[str, List[str]]]` | `None`                   | 最多 4 个序列，达到这些序列时 API 将停止生成后续 token。                        |
| `seed`              | `Optional[int]`               | `None`                       | 用于确定性采样的种子。                                                           |
| `request_params`    | `Optional[Dict[str, Any]]`    | `None`                       | 合并到请求中的额外关键字参数。                                                |
| `client_params`     | `Optional[Dict[str, Any]]`    | `None`                       | 要传递给客户端的其他参数。                                                 |
| `timeout`           | `Optional[float]`             | `None`                       | HTTP 请求的超时时间。                                                                 |
| `max_retries`       | `Optional[int]`               | `None`                       | 请求最大重试次数。                                                            |
| `enable_thinking`   | `Optional[bool]`              | `None`                       | 启用 vLLM 的“思考”模式 (在 `chat_template_kwargs` 中传递 `enable_thinking`)。            |

<Note>
`vLLM` 还支持 [OpenAI](/reference/models/openai) 的参数。
</Note>