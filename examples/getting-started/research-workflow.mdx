---
title: ç ”ç©¶å·¥ä½œæµç¨‹
---

æœ¬ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•æ„å»ºä¸€ä¸ªå¤æ‚çš„ç ”ç©¶å·¥ä½œæµç¨‹ï¼Œè¯¥æµç¨‹ç»“åˆäº†ï¼š
ğŸ” ç”¨äºæŸ¥æ‰¾ç›¸å…³èµ„æºçš„ç½‘ç»œæœç´¢èƒ½åŠ›
ğŸ“š å†…å®¹æå–å’Œå¤„ç†
âœï¸ å­¦æœ¯é£æ ¼æŠ¥å‘Šç”Ÿæˆ
ğŸ’¾ æ™ºèƒ½ç¼“å­˜ä»¥æé«˜æ€§èƒ½

æˆ‘ä»¬ä½¿ç”¨äº†ä»¥ä¸‹å…è´¹å·¥å…·ï¼š
- DuckDuckGoToolsï¼šæœç´¢ç›¸å…³æ–‡ç« çš„ç½‘ç»œ
- Newspaper4kToolsï¼šæŠ“å–å’Œå¤„ç†æ–‡ç« å†…å®¹

æ‚¨å¯ä»¥å°è¯•ä»¥ä¸‹ç¤ºä¾‹ç ”ç©¶ä¸»é¢˜ï¼š
- â€œé‡å­è®¡ç®—çš„æœ€æ–°è¿›å±•æ˜¯ä»€ä¹ˆï¼Ÿâ€
- â€œç ”ç©¶äººå·¥æ™ºèƒ½æ„è¯†çš„å½“å‰çŠ¶æ€â€
- â€œåˆ†æèšå˜èƒ½æºçš„æœ€æ–°çªç ´â€
- â€œè°ƒæŸ¥å¤ªç©ºæ—…æ¸¸å¯¹ç¯å¢ƒçš„å½±å“â€
- â€œæ¢ç´¢å¯¿å‘½ç ”ç©¶çš„æœ€æ–°å‘ç°â€

## ä»£ç 

```python research_workflow.py
import json
from textwrap import dedent
from typing import Dict, Iterator, Optional

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.workflow.sqlite import SqliteWorkflowStorage
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.newspaper4k import Newspaper4kTools
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow import RunEvent, RunResponse, Workflow
from pydantic import BaseModel, Field


class Article(BaseModel):
    title: str = Field(..., description="æ–‡ç« çš„æ ‡é¢˜ã€‚")
    url: str = Field(..., description="æ–‡ç« çš„é“¾æ¥ã€‚")
    summary: Optional[str] = Field(
        ..., description="æ–‡ç« çš„æ‘˜è¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰ã€‚"
    )


class SearchResults(BaseModel):
    articles: list[Article]


class ScrapedArticle(BaseModel):
    title: str = Field(..., description="æ–‡ç« çš„æ ‡é¢˜ã€‚")
    url: str = Field(..., description="æ–‡ç« çš„é“¾æ¥ã€‚")
    summary: Optional[str] = Field(
        ..., description="æ–‡ç« çš„æ‘˜è¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰ã€‚"
    )
    content: Optional[str] = Field(
        ...,
        description="æ–‡ç« çš„å†…å®¹ï¼ˆå¦‚æœå¯ç”¨ï¼Œä»¥ Markdown æ ¼å¼ï¼‰ã€‚å¦‚æœå†…å®¹ä¸å¯ç”¨æˆ–æ— æ„ä¹‰ï¼Œåˆ™è¿”å› Noneã€‚",
    )


class ResearchReportGenerator(Workflow):
    description: str = dedent("""\
    ç”Ÿæˆå…¨é¢çš„ç ”ç©¶æŠ¥å‘Šï¼Œç»“åˆå­¦æœ¯ä¸¥è°¨æ€§å’Œå¼•äººå…¥èƒœçš„å™äº‹é£æ ¼ã€‚
    æ­¤å·¥ä½œæµç¨‹ååŒå¤šä¸ª AI ä»£ç†æ¥æœç´¢ã€åˆ†æå’Œç»¼åˆæ¥è‡ªä¸åŒæ¥æºçš„ä¿¡æ¯ï¼Œå¹¶å°†å…¶æ•´åˆæˆç»“æ„è‰¯å¥½çš„æŠ¥å‘Šã€‚
    """)

    web_searcher: Agent = Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        tools=[DuckDuckGoTools()],
        description=dedent("""\
        æ‚¨æ˜¯ ResearchBot-Xï¼Œæ˜¯å‘ç°å’Œè¯„ä¼°å­¦æœ¯å’Œç§‘å­¦æ¥æºçš„ä¸“å®¶ã€‚\
        """),
        instructions=dedent("""\
        ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„ç ”ç©¶åŠ©ç†ï¼Œç²¾é€šæ¥æºè¯„ä¼°ï¼ğŸ”
        æœç´¢ 10-15 ä¸ªèµ„æºï¼Œå¹¶ç¡®å®šå…¶ä¸­ 5-7 ä¸ªæœ€æƒå¨ã€æœ€ç›¸å…³çš„èµ„æºã€‚
        ä¼˜å…ˆè€ƒè™‘ï¼š
        - åŒè¡Œè¯„å®¡çš„æ–‡ç« å’Œå­¦æœ¯å‡ºç‰ˆç‰©
        - æ¥è‡ªçŸ¥åæœºæ„çš„æœ€æ–°è¿›å±•
        - æƒå¨æ–°é—»æ¥æºå’Œä¸“å®¶è¯„è®º
        - æ¥è‡ªå…¬è®¤ä¸“å®¶çš„ä¸åŒè§‚ç‚¹
        é¿å…è§‚ç‚¹æ€§æ–‡ç« å’Œéæƒå¨æ¥æºã€‚\
        """),
        response_model=SearchResults,
    )

    article_scraper: Agent = Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        tools=[Newspaper4kTools()],
        description=dedent("""\
        æ‚¨æ˜¯ ContentBot-Xï¼Œæ˜¯æå–å’Œæ„å»ºå­¦æœ¯å†…å®¹çš„ä¸“å®¶ã€‚\
        """),
        instructions=dedent("""\
        ä½ æ˜¯ä¸€ä½æ³¨é‡å­¦æœ¯ç»†èŠ‚çš„ç²¾å‡†å†…å®¹ç­–å±•äººï¼ğŸ“š
        å¤„ç†å†…å®¹æ—¶ï¼š
           - ä»æ–‡ç« ä¸­æå–å†…å®¹
           - ä¿ç•™å­¦æœ¯å¼•æ–‡å’Œå‚è€ƒæ–‡çŒ®
           - åœ¨æœ¯è¯­ä¸Šä¿æŒæŠ€æœ¯å‡†ç¡®æ€§
           - ä»¥æ¸…æ™°çš„ç« èŠ‚é€»è¾‘ç»„ç»‡å†…å®¹
           - æå–å…³é”®å‘ç°å’Œæ–¹æ³•ç»†èŠ‚
           - ä¼˜é›…åœ°å¤„ç†ä»˜è´¹å¢™å†…å®¹
        å°†æ‰€æœ‰å†…å®¹æ ¼å¼åŒ–ä¸ºå¹²å‡€çš„ Markdownï¼Œä»¥è·å¾—æœ€ä½³å¯è¯»æ€§ã€‚\
        """),
        response_model=ScrapedArticle,
    )

    writer: Agent = Agent(
        model=OpenAIChat(id="gpt-4o"),
        description=dedent("""\
        æ‚¨æ˜¯ Professor X-2000ï¼Œä¸€ä½æ°å‡ºçš„ AI ç ”ç©¶ç§‘å­¦å®¶ï¼Œç»“åˆäº†å­¦æœ¯ä¸¥è°¨æ€§å’Œå¼•äººå…¥èƒœçš„å™äº‹é£æ ¼ã€‚\
        """),
        instructions=dedent("""\
        å‘æŒ¥ä¸–ç•Œçº§å­¦æœ¯ç ”ç©¶è€…çš„ä¸“ä¸šçŸ¥è¯†ï¼
        ğŸ¯ åˆ†æé˜¶æ®µï¼š
          - è¯„ä¼°æ¥æºçš„å¯ä¿¡åº¦å’Œç›¸å…³æ€§
          - äº¤å‰å¼•ç”¨æ¥æºé—´çš„å‘ç°
          - è¯†åˆ«å…³é”®ä¸»é¢˜å’Œçªç ´
        ğŸ’¡ ç»¼åˆé˜¶æ®µï¼š
          - å¼€å‘è¿è´¯çš„å™äº‹æ¡†æ¶
          - è¿æ¥åˆ†æ•£çš„å‘ç°
          - çªå‡ºçŸ›ç›¾æˆ–å·®è·
        âœï¸ å†™ä½œé˜¶æ®µï¼š
          - ä»å¼•äººå…¥èƒœçš„æ‰§è¡Œæ‘˜è¦å¼€å§‹ï¼Œå¸å¼•è¯»è€…
          - æ¸…æ™°åœ°å‘ˆç°å¤æ‚æ€æƒ³
          - æ”¯æŒæ‰€æœ‰è®ºç‚¹å¹¶é™„å¸¦å¼•ç”¨
          - å¹³è¡¡æ·±åº¦ä¸å¯è¯»æ€§
          - åœ¨ä¿æŒå­¦æœ¯è¯­è°ƒçš„åŒæ—¶ç¡®ä¿æ˜“è¯»æ€§
          - ä»¥å½±å“å’Œæœªæ¥æ–¹å‘ç»“å°¾ã€‚\
        """),
        expected_output=dedent("""\
        # {å¼•äººå…¥èƒœçš„å­¦æœ¯æ ‡é¢˜}

        ## æ‰§è¡Œæ‘˜è¦
        {å…³é”®å‘ç°å’Œæ„ä¹‰çš„ç®€æ´æ¦‚è¿°}

        ## å¼•è¨€
        {ç ”ç©¶èƒŒæ™¯å’Œä¸Šä¸‹æ–‡}
        {è¯¥é¢†åŸŸçš„å½“å‰çŠ¶å†µ}

        ## æ–¹æ³•
        {æœç´¢å’Œåˆ†ææ–¹æ³•}
        {æ¥æºè¯„ä¼°æ ‡å‡†}

        ## ä¸»è¦å‘ç°
        {ä¸»è¦å‘ç°å’Œè¿›å±•}
        {æ”¯æŒè¯æ®å’Œåˆ†æ}
        {å¯¹æ¯”è§‚ç‚¹}

        ## åˆ†æ
        {å¯¹å‘ç°çš„æ‰¹åˆ¤æ€§è¯„ä¼°}
        {æ•´åˆå¤šæ–¹é¢è§‚ç‚¹}
        {æ¨¡å¼å’Œè¶‹åŠ¿è¯†åˆ«}

        ## å½±å“
        {å­¦æœ¯å’Œå®è·µæ„ä¹‰}
        {æœªæ¥ç ”ç©¶æ–¹å‘}
        {æ½œåœ¨åº”ç”¨}

        ## è¦ç‚¹æ€»ç»“
        - {å…³é”®å‘ç° 1}
        - {å…³é”®å‘ç° 2}
        - {å…³é”®å‘ç° 3}

        ## å‚è€ƒæ–‡çŒ®
        {æ ¼å¼æ­£ç¡®çš„å­¦æœ¯å¼•ç”¨}

        ---
        æœ¬æŠ¥å‘Šç”± Professor X-2000 ç”Ÿæˆ
        é«˜çº§ç ”ç©¶éƒ¨
        æ—¥æœŸï¼š{å½“å‰æ—¥æœŸ}\
        """),
        markdown=True,
    )

    def run(
        self,
        topic: str,
        use_search_cache: bool = True,
        use_scrape_cache: bool = True,
        use_cached_report: bool = True,
    ) -> Iterator[RunResponse]:
        """
        é’ˆå¯¹ç»™å®šä¸»é¢˜ç”Ÿæˆå…¨é¢çš„æ–°é—»æŠ¥å‘Šã€‚

        æ­¤å‡½æ•°åè°ƒä¸€ä¸ªå·¥ä½œæµç¨‹æ¥æœç´¢æ–‡ç« ã€æŠ“å–å…¶å†…å®¹å¹¶ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šã€‚
        å®ƒåˆ©ç”¨ç¼“å­˜æœºåˆ¶æ¥ä¼˜åŒ–æ€§èƒ½ã€‚

        Args:
            topic (str): è¦ç”Ÿæˆæ–°é—»æŠ¥å‘Šçš„ä¸»é¢˜ã€‚
            use_search_cache (bool, optional): æ˜¯å¦ä½¿ç”¨ç¼“å­˜çš„æœç´¢ç»“æœã€‚é»˜è®¤ä¸º Trueã€‚
            use_scrape_cache (bool, optional): æ˜¯å¦ä½¿ç”¨ç¼“å­˜çš„æ–‡ç« æŠ“å–ç»“æœã€‚é»˜è®¤ä¸º Trueã€‚
            use_cached_report (bool, optional): æ˜¯å¦è¿”å›ä¹‹å‰å·²ç”Ÿæˆçš„åŒä¸€ä¸»é¢˜çš„æŠ¥å‘Šã€‚é»˜è®¤ä¸º Falseã€‚

        Returns:
            Iterator[RunResponse]: ä¸€ä¸ªåŒ…å«ç”Ÿæˆçš„æŠ¥å‘Šæˆ–çŠ¶æ€ä¿¡æ¯çš„å¯¹è±¡æµã€‚

        æ­¥éª¤ï¼š
        1. å¦‚æœ use_cached_report ä¸º Trueï¼Œåˆ™æ£€æŸ¥ç¼“å­˜æŠ¥å‘Šã€‚
        2. æœç´¢å…³äºè¯¥ä¸»é¢˜çš„æ–‡ç« ï¼š
            - å¦‚æœç¼“å­˜çš„æœç´¢ç»“æœå¯ç”¨ä¸” use_search_cache ä¸º Trueï¼Œåˆ™ä½¿ç”¨å®ƒä»¬ã€‚
            - å¦åˆ™ï¼Œæ‰§è¡Œæ–°çš„ç½‘ç»œæœç´¢ã€‚
        3. æŠ“å–æ¯ç¯‡æ–‡ç« çš„å†…å®¹ï¼š
            - å¦‚æœç¼“å­˜çš„æ–‡ç« æŠ“å–ç»“æœå¯ç”¨ä¸” use_scrape_cache ä¸º Trueï¼Œåˆ™ä½¿ç”¨å®ƒä»¬ã€‚
            - æŠ“å–ä¸åœ¨ç¼“å­˜ä¸­çš„æ–°æ–‡ç« ã€‚
        4. ä½¿ç”¨æŠ“å–åˆ°çš„æ–‡ç« å†…å®¹ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šã€‚

        è¯¥å‡½æ•°åˆ©ç”¨ `session_state` æ¥å­˜å‚¨å’Œæ£€ç´¢ç¼“å­˜æ•°æ®ã€‚
        """
        logger.info(f"æ­£åœ¨ç”Ÿæˆå…³äºä»¥ä¸‹ä¸»é¢˜çš„æŠ¥å‘Šï¼š{topic}")

        # å¦‚æœ use_cached_report ä¸º Trueï¼Œåˆ™ä½¿ç”¨ç¼“å­˜çš„æŠ¥å‘Š
        if use_cached_report:
            cached_report = self.get_cached_report(topic)
            if cached_report:
                yield RunResponse(
                    content=cached_report, event=RunEvent.workflow_completed
                )
                return

        # æœç´¢å…³äºè¯¥ä¸»é¢˜çš„æ–‡ç« 
        search_results: Optional[SearchResults] = self.get_search_results(
            topic, use_search_cache
        )
        # å¦‚æœæœªæ‰¾åˆ°è¯¥ä¸»é¢˜çš„ search_resultsï¼Œåˆ™ç»“æŸå·¥ä½œæµç¨‹
        if search_results is None or len(search_results.articles) == 0:
            yield RunResponse(
                event=RunEvent.workflow_completed,
                content=f"æŠ±æ­‰ï¼Œæœªèƒ½æ‰¾åˆ°å…³äºä¸»é¢˜ '{topic}' çš„ä»»ä½•æ–‡ç« ",
            )
            return

        # æŠ“å–æœç´¢ç»“æœ
        scraped_articles: Dict[str, ScrapedArticle] = self.scrape_articles(
            search_results, use_scrape_cache
        )

        # æ’°å†™ç ”ç©¶æŠ¥å‘Š
        yield from self.write_research_report(topic, scraped_articles)

    def get_cached_report(self, topic: str) -> Optional[str]:
        logger.info("æ­£åœ¨æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç¼“å­˜çš„æŠ¥å‘Š")
        return self.session_state.get("reports", {}).get(topic)

    def add_report_to_cache(self, topic: str, report: str):
        logger.info(f"æ­£åœ¨ä¸ºä¸»é¢˜ä¿å­˜æŠ¥å‘Šï¼š{topic}")
        self.session_state.setdefault("reports", {})
        self.session_state["reports"][topic] = report
        # å°†æŠ¥å‘Šä¿å­˜åˆ°å­˜å‚¨
        self.write_to_storage()

    def get_cached_search_results(self, topic: str) -> Optional[SearchResults]:
        logger.info("æ­£åœ¨æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç¼“å­˜çš„æœç´¢ç»“æœ")
        return self.session_state.get("search_results", {}).get(topic)

    def add_search_results_to_cache(self, topic: str, search_results: SearchResults):
        logger.info(f"æ­£åœ¨ä¸ºä¸»é¢˜ä¿å­˜æœç´¢ç»“æœï¼š{topic}")
        self.session_state.setdefault("search_results", {})
        self.session_state["search_results"][topic] = search_results.model_dump()
        # å°†æœç´¢ç»“æœä¿å­˜åˆ°å­˜å‚¨
        self.write_to_storage()

    def get_cached_scraped_articles(
        self, topic: str
    ) -> Optional[Dict[str, ScrapedArticle]]:
        logger.info("æ­£åœ¨æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç¼“å­˜çš„æ–‡ç« æŠ“å–ç»“æœ")
        return self.session_state.get("scraped_articles", {}).get(topic)

    def add_scraped_articles_to_cache(
        self, topic: str, scraped_articles: Dict[str, ScrapedArticle]
    ):
        logger.info(f"æ­£åœ¨ä¸ºä¸»é¢˜ä¿å­˜æ–‡ç« æŠ“å–ç»“æœï¼š{topic}")
        self.session_state.setdefault("scraped_articles", {})
        self.session_state["scraped_articles"][topic] = scraped_articles
        # å°†æ–‡ç« æŠ“å–ç»“æœä¿å­˜åˆ°å­˜å‚¨
        self.write_to_storage()

    def get_search_results(
        self, topic: str, use_search_cache: bool, num_attempts: int = 3
    ) -> Optional[SearchResults]:
        # å¦‚æœ use_search_cache ä¸º Trueï¼Œåˆ™ä»ä¼šè¯çŠ¶æ€è·å–ç¼“å­˜çš„ search_results
        if use_search_cache:
            try:
                search_results_from_cache = self.get_cached_search_results(topic)
                if search_results_from_cache is not None:
                    search_results = SearchResults.model_validate(
                        search_results_from_cache
                    )
                    logger.info(
                        f"ä»ç¼“å­˜ä¸­æ‰¾åˆ° {len(search_results.articles)} ç¯‡æ–‡ç« ã€‚"
                    )
                    return search_results
            except Exception as e:
                logger.warning(f"æ— æ³•ä»ç¼“å­˜è¯»å–æœç´¢ç»“æœï¼š{e}")

        # å¦‚æœæ²¡æœ‰ç¼“å­˜çš„ search_resultsï¼Œåˆ™ä½¿ç”¨ web_searcher æŸ¥æ‰¾æœ€æ–°æ–‡ç« 
        for attempt in range(num_attempts):
            try:
                searcher_response: RunResponse = self.web_searcher.run(topic)
                if (
                    searcher_response is not None
                    and searcher_response.content is not None
                    and isinstance(searcher_response.content, SearchResults)
                ):
                    article_count = len(searcher_response.content.articles)
                    logger.info(
                        f"ç¬¬ {attempt + 1} æ¬¡å°è¯•æ‰¾åˆ° {article_count} ç¯‡æ–‡ç« "
                    )
                    # ç¼“å­˜æœç´¢ç»“æœ
                    self.add_search_results_to_cache(topic, searcher_response.content)
                    return searcher_response.content
                else:
                    logger.warning(
                        f"ç¬¬ {attempt + 1}/{num_attempts} æ¬¡å°è¯•å¤±è´¥ï¼šå“åº”ç±»å‹æ— æ•ˆ"
                    )
            except Exception as e:
                logger.warning(f"ç¬¬ {attempt + 1}/{num_attempts} æ¬¡å°è¯•å¤±è´¥ï¼š{str(e)}")

        logger.error(f"åœ¨ {num_attempts} æ¬¡å°è¯•åæœªèƒ½è·å–æœç´¢ç»“æœ")
        return None

    def scrape_articles(
        self, search_results: SearchResults, use_scrape_cache: bool
    ) -> Dict[str, ScrapedArticle]:
        scraped_articles: Dict[str, ScrapedArticle] = {}

        # å¦‚æœ use_scrape_cache ä¸º Trueï¼Œåˆ™ä»ä¼šè¯çŠ¶æ€è·å–ç¼“å­˜çš„ scraped_articles
        if use_scrape_cache:
            try:
                scraped_articles_from_cache = self.get_cached_scraped_articles(topic)
                if scraped_articles_from_cache is not None:
                    scraped_articles = scraped_articles_from_cache
                    logger.info(
                        f"ä»ç¼“å­˜ä¸­æ‰¾åˆ° {len(scraped_articles)} ç¯‡æŠ“å–æ–‡ç« ã€‚"
                    )
                    return scraped_articles
            except Exception as e:
                logger.warning(f"æ— æ³•ä»ç¼“å­˜è¯»å–æŠ“å–æ–‡ç« ï¼š{e}")

        # æŠ“å–ä¸åœ¨ç¼“å­˜ä¸­çš„æ–‡ç« 
        for article in search_results.articles:
            if article.url in scraped_articles:
                logger.info(f"åœ¨ç¼“å­˜ä¸­æ‰¾åˆ°æŠ“å–æ–‡ç« ï¼š{article.url}")
                continue

            article_scraper_response: RunResponse = self.article_scraper.run(
                article.url
            )
            if (
                article_scraper_response is not None
                and article_scraper_response.content is not None
                and isinstance(article_scraper_response.content, ScrapedArticle)
            ):
                scraped_articles[article_scraper_response.content.url] = (
                    article_scraper_response.content
                )
                logger.info(f"æŠ“å–æ–‡ç« ï¼š{article_scraper_response.content.url}")

        # å°†æŠ“å–æ–‡ç« ä¿å­˜åˆ°ä¼šè¯çŠ¶æ€
        self.add_scraped_articles_to_cache(topic, scraped_articles)
        return scraped_articles

    def write_research_report(
        self, topic: str, scraped_articles: Dict[str, ScrapedArticle]
    ) -> Iterator[RunResponse]:
        logger.info("æ­£åœ¨æ’°å†™ç ”ç©¶æŠ¥å‘Š")
        # å‡†å¤‡å†™å…¥å™¨çš„è¾“å…¥
        writer_input = {
            "topic": topic,
            "articles": [v.model_dump() for v in scraped_articles.values()],
        }
        # è¿è¡Œå†™å…¥å™¨å¹¶ç”Ÿæˆå“åº”
        yield from self.writer.run(json.dumps(writer_input, indent=4), stream=True)
        # å°†ç ”ç©¶æŠ¥å‘Šä¿å­˜åˆ°ç¼“å­˜
        self.add_report_to_cache(topic, self.writer.run_response.content)


# å¦‚æœè„šæœ¬ç›´æ¥æ‰§è¡Œï¼Œåˆ™è¿è¡Œå·¥ä½œæµç¨‹
if __name__ == "__main__":
    from rich.prompt import Prompt

    # ç¤ºä¾‹ç ”ç©¶ä¸»é¢˜
    example_topics = [
        "2024å¹´é‡å­è®¡ç®—çªç ´",
        "äººå·¥æ™ºèƒ½æ„è¯†ç ”ç©¶",
        "èšå˜èƒ½æºå‘å±•",
        "å¤ªç©ºæ—…æ¸¸å¯¹ç¯å¢ƒçš„å½±å“",
        "å¯¿å‘½ç ”ç©¶è¿›å±•",
    ]

    topics_str = "\n".join(
        f"{i + 1}. {topic}" for i, topic in enumerate(example_topics)
    )

    print(f"\nğŸ“š ç¤ºä¾‹ç ”ç©¶ä¸»é¢˜:\n{topics_str}\n")

    # ä»ç”¨æˆ·é‚£é‡Œè·å–ä¸»é¢˜
    topic = Prompt.ask(
        "[bold]è¯·è¾“å…¥ç ”ç©¶ä¸»é¢˜[/bold]\nâœ¨",
        default="2024å¹´é‡å­è®¡ç®—çªç ´",
    )

    # å°†ä¸»é¢˜è½¬æ¢ä¸º URL å®‰å…¨å­—ç¬¦ä¸²ï¼Œç”¨äº session_id
    url_safe_topic = topic.lower().replace(" ", "-")

    # åˆå§‹åŒ–æ–°é—»æŠ¥å‘Šç”Ÿæˆå·¥ä½œæµç¨‹
    generate_research_report = ResearchReportGenerator(
        session_id=f"generate-report-on-{url_safe_topic}",
        storage=SqliteWorkflowStorage(
            table_name="generate_research_report_workflow",
            db_file="tmp/workflows.db",
        ),
    )

    # æ‰§è¡Œå¯ç”¨ç¼“å­˜çš„å·¥ä½œæµç¨‹
    report_stream: Iterator[RunResponse] = generate_research_report.run(
        topic=topic,
        use_search_cache=True,
        use_scrape_cache=True,
        use_cached_report=True,
    )

    # æ‰“å°å“åº”
    pprint_run_response(report_stream, markdown=True)
```

## ç”¨æ³•

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="å®‰è£…åº“">
    ```bash
    pip install openai duckduckgo-search newspaper4k lxml_html_clean sqlalchemy agno
    ```
  </Step>

  <Step title="è¿è¡Œå·¥ä½œæµç¨‹">
    ```bash
    python research_workflow.py
    ```
  </Step>

</Steps>