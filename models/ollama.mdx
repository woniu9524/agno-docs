---
title: Ollama
description: 学习如何与 Agno 一起使用 Ollama。
---

本地运行大型语言模型，使用 Ollama

[Ollama](https://ollama.com) 是一个在本地运行模型的绝佳工具。

Ollama 支持多种开源模型。在此处 ([https://ollama.com/library](https://ollama.com/library)) 查看模型库。

我们建议进行实验，找到最适合您用例的模型。以下是一些通用建议：

- `llama3.3` 模型适合大多数基本用例。
- `qwen` 模型在工具使用方面表现尤其出色。
- `deepseek-r1` 模型具有强大的推理能力。
- `phi4` 模型功能强大，但体积小巧。

## 设置模型

安装 [ollama](https://ollama.com) 并运行模型

```bash run model
ollama run llama3.1
```

这将为您提供与模型的交互式会话。

或者，下载模型以在 Agno 代理中使用

```bash pull model
ollama pull llama3.1
```

## 示例

在本地拥有模型后，使用 `Ollama` 模型类来访问它

<CodeGroup>

```python agent.py
from agno.agent import Agent, RunResponse
from agno.models.ollama import Ollama

agent = Agent(
    model=Ollama(id="llama3.1"),
    markdown=True
)

# 在终端中打印响应
agent.print_response("创作一个两句话的恐怖故事。")
```

</CodeGroup>

<Note> 在此处 ([../examples/models/ollama](../examples/models/ollama)) 查看更多示例。 </Note>

## 参数

<Snippet file="model-ollama-params.mdx" />

`Ollama` 是 [Model](/reference/models/model) 类的子类，可以访问相同的参数。