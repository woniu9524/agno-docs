---
title: vLLM
---
[vLLM](https://docs.vllm.ai/en/latest/) 是一个快速易用的 LLM 推理和服务库，专为高吞吐量和内存高效的 LLM 服务设计。

## 先决条件

安装 vLLM 并开始服务模型：

```bash install vLLM
pip install vllm
```

```bash start vLLM server
vllm serve Qwen/Qwen2.5-7B-Instruct \
    --enable-auto-tool-choice \
    --tool-call-parser hermes \
    --dtype float16 \
    --max-model-len 8192 \
    --gpu-memory-utilization 0.9
```

这将启动一个具有 OpenAI 兼容 API 的 vLLM 服务器。

<Note>
  默认的 vLLM 服务器 URL 是 `http://localhost:8000/`
</Note>

## 示例

基本 Agent
<CodeGroup>

```python agent.py
from agno.agent import Agent
from agno.models.vllm import vLLM

agent = Agent(
    model=vLLM(
        id="meta-llama/Llama-3.1-8B-Instruct", 
        base_url="http://localhost:8000/",
    ),
    markdown=True
)

agent.print_response("Share a 2 sentence horror story.")
```

</CodeGroup>

## 高级用法

### 使用工具

vLLM 模型可与 Agno 工具无缝协同工作：

```python with_tools.py
from agno.agent import Agent
from agno.models.vllm import vLLM
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=vLLM(id="meta-llama/Llama-3.1-8B-Instruct"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True
)

agent.print_response("What's the latest news about AI?")
```

<Note> 在此处查看更多示例：[here](../examples/models/vllm)。 </Note>

支持模型的完整列表，请参阅 [vLLM 文档](https://docs.vllm.ai/en/latest/models/supported_models.html)。

## 参数

<Snippet file="model-vllm-params.mdx" />

`vLLM` 是 [Model](/reference/models/model) 类的子类，可以访问相同的参数。